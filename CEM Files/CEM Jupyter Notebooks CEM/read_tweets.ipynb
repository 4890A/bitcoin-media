{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import TwythonStreamer\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from textblob import TextBlob \n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet): \n",
    "         \n",
    "        #Utility function to clean tweet text by removing links, special characters \n",
    "        # using simple regex statements. \n",
    "        \n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_sentiment(tweet): \n",
    "    #Utility function to classify sentiment of passed tweet \n",
    "    # using textblob's sentiment method \n",
    "        \n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(clean_tweet(tweet)) \n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unwanted data\n",
    "def process_tweet(tweet):\n",
    "    d = {}\n",
    "    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]\n",
    "    d['text'] = clean_tweet(tweet['text'])\n",
    "    d['user'] = tweet['user']['screen_name']\n",
    "    d['user_loc'] = tweet['user']['location']\n",
    "    d['date']= tweet['created_at']\n",
    "    #d['sentiment'] = get_tweet_sentiment(clean_tweet['text']))\n",
    "    d['sentiment'] = get_tweet_sentiment(tweet['text'])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that inherits TwythonStreamer\n",
    "class MyStreamer(TwythonStreamer):     \n",
    "\n",
    "    # Received data\n",
    "    def on_success(self, data):\n",
    "\n",
    "        # Only collect tweets in English\n",
    "        if data['lang'] == 'en':\n",
    "            tweet_data = process_tweet(data)\n",
    "            self.save_to_csv(tweet_data)\n",
    "\n",
    "    # Problem with the API\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "        \n",
    "    # Save each tweet to csv file\n",
    "    def save_to_csv(self, tweet):\n",
    "        with open(r'saved_tweets.csv', 'a', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(list(tweet.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate from our streaming class\n",
    "stream = MyStreamer(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'], \n",
    "                    creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])\n",
    "# Start the stream\n",
    "\n",
    "with open(r'saved_tweets.csv', 'a', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['hashtags','text', 'user', 'user_loc', 'date', 'sentiment'])\n",
    "\n",
    "topic = \"BTC\"\n",
    "\n",
    "stream.statuses.filter(track='BTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>Great moments in crypto</td>\n",
       "      <td>rod3000</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Tue Sep 17 01:36:31 +0000 2019</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>RT Fancy a slice of 10 000 in Bitcoin We re le...</td>\n",
       "      <td>Annabelbeyerfr1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Sep 17 01:36:33 +0000 2019</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['brand', 'tech', 'technology', 'innovation', ...</td>\n",
       "      <td>RT on SALE brand tech technology innovation bi...</td>\n",
       "      <td>SantchiWeb</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>Tue Sep 17 01:36:34 +0000 2019</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hashtags  \\\n",
       "0                                                 []   \n",
       "1                                                NaN   \n",
       "2                                                 []   \n",
       "3                                                NaN   \n",
       "4  ['brand', 'tech', 'technology', 'innovation', ...   \n",
       "\n",
       "                                                text             user  \\\n",
       "0                            Great moments in crypto          rod3000   \n",
       "1                                                NaN              NaN   \n",
       "2  RT Fancy a slice of 10 000 in Bitcoin We re le...  Annabelbeyerfr1   \n",
       "3                                                NaN              NaN   \n",
       "4  RT on SALE brand tech technology innovation bi...       SantchiWeb   \n",
       "\n",
       "  user_loc                            date sentiment  \n",
       "0   Sydney  Tue Sep 17 01:36:31 +0000 2019  positive  \n",
       "1      NaN                             NaN       NaN  \n",
       "2      NaN  Tue Sep 17 01:36:33 +0000 2019  positive  \n",
       "3      NaN                             NaN       NaN  \n",
       "4   Wirral  Tue Sep 17 01:36:34 +0000 2019   neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "tweets = pd.read_csv(\"saved_tweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>Great moments in crypto</td>\n",
       "      <td>rod3000</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Tue Sep 17 01:36:31 +0000 2019</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>RT Fancy a slice of 10 000 in Bitcoin We re le...</td>\n",
       "      <td>Annabelbeyerfr1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Sep 17 01:36:33 +0000 2019</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['brand', 'tech', 'technology', 'innovation', ...</td>\n",
       "      <td>RT on SALE brand tech technology innovation bi...</td>\n",
       "      <td>SantchiWeb</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>Tue Sep 17 01:36:34 +0000 2019</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hashtags  \\\n",
       "0                                                 []   \n",
       "1                                                NaN   \n",
       "2                                                 []   \n",
       "3                                                NaN   \n",
       "4  ['brand', 'tech', 'technology', 'innovation', ...   \n",
       "\n",
       "                                                text             user  \\\n",
       "0                            Great moments in crypto          rod3000   \n",
       "1                                                NaN              NaN   \n",
       "2  RT Fancy a slice of 10 000 in Bitcoin We re le...  Annabelbeyerfr1   \n",
       "3                                                NaN              NaN   \n",
       "4  RT on SALE brand tech technology innovation bi...       SantchiWeb   \n",
       "\n",
       "  user_loc                            date sentiment  \n",
       "0   Sydney  Tue Sep 17 01:36:31 +0000 2019  positive  \n",
       "1      NaN                             NaN       NaN  \n",
       "2      NaN  Tue Sep 17 01:36:33 +0000 2019  positive  \n",
       "3      NaN                             NaN       NaN  \n",
       "4   Wirral  Tue Sep 17 01:36:34 +0000 2019   neutral  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>Great moments in crypto</td>\n",
       "      <td>rod3000</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Tue Sep 17 01:36:31 +0000 2019</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['brand', 'tech', 'technology', 'innovation', ...</td>\n",
       "      <td>RT on SALE brand tech technology innovation bi...</td>\n",
       "      <td>SantchiWeb</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>Tue Sep 17 01:36:34 +0000 2019</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>RT A testing bug on Kraken let users buy and s...</td>\n",
       "      <td>Cryptofever5</td>\n",
       "      <td>South East Asia</td>\n",
       "      <td>Tue Sep 17 01:36:41 +0000 2019</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>I ll sell my Bitcoin when Neveruary 1 st comes...</td>\n",
       "      <td>ClaudiwNeamtu</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>Tue Sep 17 01:36:43 +0000 2019</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[]</td>\n",
       "      <td>Bitcoin s 30 Day Volatility Just Reached Its L...</td>\n",
       "      <td>CharlesLBovaird</td>\n",
       "      <td>Boston MA</td>\n",
       "      <td>Tue Sep 17 01:36:48 +0000 2019</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             hashtags  \\\n",
       "0                                                  []   \n",
       "4   ['brand', 'tech', 'technology', 'innovation', ...   \n",
       "6                                                  []   \n",
       "8                                                  []   \n",
       "10                                                 []   \n",
       "\n",
       "                                                 text             user  \\\n",
       "0                             Great moments in crypto          rod3000   \n",
       "4   RT on SALE brand tech technology innovation bi...       SantchiWeb   \n",
       "6   RT A testing bug on Kraken let users buy and s...     Cryptofever5   \n",
       "8   I ll sell my Bitcoin when Neveruary 1 st comes...    ClaudiwNeamtu   \n",
       "10  Bitcoin s 30 Day Volatility Just Reached Its L...  CharlesLBovaird   \n",
       "\n",
       "           user_loc                            date sentiment  \n",
       "0            Sydney  Tue Sep 17 01:36:31 +0000 2019  positive  \n",
       "4            Wirral  Tue Sep 17 01:36:34 +0000 2019   neutral  \n",
       "6   South East Asia  Tue Sep 17 01:36:41 +0000 2019  positive  \n",
       "8           Denmark  Tue Sep 17 01:36:43 +0000 2019   neutral  \n",
       "10        Boston MA  Tue Sep 17 01:36:48 +0000 2019   neutral  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2 = tweets.dropna()\n",
    "tweets2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765\n"
     ]
    }
   ],
   "source": [
    "print (len(list(tweets2['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnametweets = \"TWEETS2.csv\"\n",
    "tweets2.to_csv(fnametweets, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73, 411, 281]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tweets2.groupby('sentiment').count()\n",
    "c\n",
    "list(c[\"hashtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': [73], 'neutral': [411], 'positive': [281]}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = {}\n",
    "data1['negative'] = [list(c[\"hashtags\"])[0]]\n",
    "data1['neutral'] = [list(c[\"hashtags\"])[1]]\n",
    "data1['positive'] = [list(c[\"hashtags\"])[2]]\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>411</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  neutral  positive\n",
       "0        73      411       281"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(data1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save cleaned location in CSV file\n",
    "df2.to_csv(\"sentimentval.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [negative, neutral, positive]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = {'sentiment value': list(c[\"hashtags\"])}\n",
    "df1 = DataFrame(data, columns= ['negative','neutral', 'positive'])\n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sentiment1 = 'sentiment1.csv'\n",
    "with open(file_sentiment1, 'a') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['negative','neutral', 'positive'])\n",
    "    writer.writerow(list(c[\"hashtags\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(file_sentiment1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asi40\\.conda\\envs\\PythonData\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Using Nominatim with the default \"geopy/1.20.0\" `user_agent` is strongly discouraged, as it violates Nominatim's ToS https://operations.osmfoundation.org/policies/nominatim/ and may possibly cause 403 and 429 HTTP errors. Please specify a custom `user_agent` with `Nominatim(user_agent=\"my-application\")` or by overriding the default `user_agent`: `geopy.geocoders.options.default_user_agent = \"my-application\"`. In geopy 2.0 this will become an exception.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import gmplot\n",
    "\n",
    "geolocator = Nominatim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = 'tweets_location.csv'\n",
    "with open(file_location, 'a') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['location','latitude', 'longitude'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy import geocoders  \n",
    "# gn = geocoders.GeoNames(username='cmoreno2020')\n",
    "# gn.geocode(\"Cleveland, OH\", exactly_one=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydney\n",
      "Wirral\n",
      "South East Asia\n",
      "Denmark\n",
      "Boston MA\n",
      "Blockchain\n",
      "California, USA\n",
      "New Delhi, India\n",
      "New York, NY\n",
      "Global\n",
      "San Francisco\n",
      "Panama\n",
      "Sandusky, OH\n",
      "Los Angeles, CA\n",
      "BCH\n",
      "United States\n",
      "United States\n",
      "Japan\n",
      "blockchain\n",
      "0x\n",
      "Los Angeles, CA\n",
      "United States\n",
      "PORTUGUESA - GUANARE \n",
      "Dallas, TX\n",
      "Canada\n",
      "nigeria\n",
      "New Delhi, India\n",
      "Valencia, Espa√±a\n",
      "Cape Cod\n",
      "United States\n",
      "Montr√©al, Qu√©bec\n",
      "where\n",
      "Chicago, IL\n",
      "Rio de Janeiro, Brazil\n",
      "Los Angeles, CA\n",
      "Los Angeles\n",
      "Pakistan\n",
      "üïäÔ∏è\n",
      "Everywhere\n",
      "Los Angeles, CA\n",
      "California, USA\n",
      "New Delhi, India\n",
      "Nairobi, Kenya\n",
      "Australia\n",
      "SF\n",
      "North Pole, AK\n",
      "United States\n",
      "California, USA\n",
      "On Chain\n",
      "Pakistan\n",
      "Matter Doesn't Matter\n",
      "Home\n",
      "Madhya Pradesh, India\n",
      "West Lawn, Chicago\n",
      "Pakistan\n",
      "where\n",
      "Malaysia\n",
      "Georgia, USA\n",
      "S. Bridge\n",
      "California, USA\n",
      "MARS\n",
      "Espa√±a\n",
      "The World\n",
      "Appleton, WI\n",
      "Central Mississippi\n",
      "Tokyo-to, Japan\n",
      "Simi Valley, CA\n",
      "Washington, DC\n",
      "Around the World\n",
      "S. Bridge\n",
      "New York, USA\n",
      "Georgia, USA\n",
      "Lagos, Nigeria\n",
      "Nairobi, Kenya\n",
      "Bandungan ,Semarang Indonesia\n",
      "Georgia, USA\n",
      "Harrisburg, PA\n",
      "temanggung\n",
      "Pune\n",
      "Halifax, Nova Scotia, Canada\n",
      "Valencia, Espa√±a\n",
      "Bandung, Indonesia\n",
      "Siliguri 734003, India\n",
      "Tokyo\n",
      "Canada\n",
      "Austin, TX\n",
      "Pakistan\n",
      "Pakistan\n",
      "Antarctica\n",
      "Here\n",
      "Moon\n",
      "Pakistan\n",
      "earth \n",
      "Sydney, NSW, Australia\n",
      "Hudson Valley, NY\n",
      "Jakarta Selatan, DKI Jakarta\n",
      "Arizona, USA\n",
      "Florida, USA\n",
      "Brisbane, Queensland\n",
      "Blockchain\n",
      "Washington\n",
      "Global\n",
      "Norway\n",
      "Raleigh, NC\n",
      "World\n",
      "Moon\n",
      "Los Angeles, CA\n",
      "Boston, MA\n",
      "earth \n",
      "Bansur, India\n",
      "Thunder Bay, Ontario\n",
      "Miami Beach, FL\n",
      "London, England\n",
      "Las Vegas, NV\n",
      "Lagos, Nigeria\n",
      "S√£o Paulo - Brazil\n",
      "Vienna, Austria\n",
      "Roseville CA\n",
      "New York, USA\n",
      "Gadsden, AL 35901\n",
      "Earth\n",
      "Gurgaon, India\n",
      "Moon\n",
      "internet\n",
      "Earth\n",
      "West Pensacola\n",
      "Copperbelt, Kitwe\n",
      "Vi·ªát Nam\n",
      "Inglewood, CA\n",
      "Bali\n",
      "Blockchain\n",
      "Los Angeles\n",
      "Nairobi, Kenya\n",
      "Yogyakarta, Indonesia\n",
      "Singapore\n",
      "Indonesia\n",
      "Philly\n",
      "Lagos, Nigeria\n",
      "Australia\n",
      "PA\n",
      "Georgia, USA\n",
      "0x\n",
      "London, United Kingdom\n",
      "Newport Beach, CA\n",
      "Ohio, USA\n",
      "Seattle, WA\n",
      "Mombasa\n",
      "Texas, USA\n",
      "Los Angeles\n",
      "Dhaka, Bangladesh\n",
      "United States\n",
      "üëáüèæüëáüèæüëáüèæ‚öïÔ∏è\n",
      "Tokyo-to, Japan\n",
      "Toronto, Ontario\n",
      "Venezuela\n",
      "Moon\n",
      "Georgia, USA\n",
      "USA\n",
      "London, England\n",
      "Abuja, Nigeria\n",
      "Makati City, National Capital \n",
      "Planet Earth\n",
      "Cyber Space\n",
      "Cyber Space\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "San Francisco, CA\n",
      "South Africa\n",
      "Florida, USA\n",
      "Earth\n",
      "San Francisco, CA\n",
      "Brasil\n",
      "Brasil\n",
      "H·∫£i Ph√≤ng, Vi·ªát Nam\n",
      "Canada\n",
      "Maracay, Venezuela\n",
      "Mooney Mooney Creek, New South Wales\n",
      "Venezuela\n",
      "M√©rida\n",
      "Santa Monica\n",
      "Earth\n",
      "Indonesia\n",
      "World Citizen\n",
      "On Chain\n",
      "USA\n",
      "japan\n",
      "Colorado\n",
      "Bakersfield, CA\n",
      "Republic of the Philippines\n",
      "Toronto, Ontario\n",
      "Xanadu\n",
      "Nunya\n",
      "Republic of the Philippines\n",
      "Vancouver, Main and Broadway\n",
      "Boston, MA\n",
      "Toronto, Ontario\n",
      "New York, USA\n",
      "Corleone, Sicilia\n",
      "Albuquerque\n",
      "Moon\n",
      "United States\n",
      "Bakersfield, CA\n",
      "Los Angeles, CA\n",
      "Indonesia\n",
      "Adelaide, South Australia\n",
      "Cincinnati USA\n",
      "On chain\n",
      "Austin, TX\n",
      "The Internet\n",
      "The Internet\n",
      "The Internet\n",
      "Colorado, USA\n",
      "PLANET EARTH\n",
      "United States\n",
      "World\n",
      "World\n",
      "World\n",
      "United States\n",
      "World\n",
      "Buenos Aires, Mexico\n",
      "New York, NY\n",
      "Bangladesh\n",
      "Austin, TX\n",
      "United States\n",
      "Bangladesh\n",
      "Los Angeles, CA\n",
      "The World\n",
      "Tokyo-to, Japan\n",
      "Baja California Sur, M√©xico\n",
      "Brooklyn\n",
      "Georgia, USA\n",
      "Antarctica\n",
      "johannesburg, south africa\n",
      "INDIA TUTICORIN \n",
      "Boston, MA\n",
      "United States\n",
      "Venezuela\n",
      "Uni\n",
      "INDIA TUTICORIN \n",
      "Moon\n",
      "Philadelphia, PA\n",
      "United States\n",
      "EU\n",
      "Internet\n",
      "Boston, MA\n",
      "New York\n",
      "United States\n",
      "Republic of the Philippines\n",
      "Êó•Êú¨\n",
      "Virginia Beach, VA\n",
      "Bengaluru\n",
      "Virginia Beach, VA\n",
      "Shermer, IL\n",
      "District of Columbia, USA\n",
      "C·∫ßn Th∆°, Vi·ªát Nam\n",
      "Show-Me State\n",
      "Kingsgate, WA\n",
      "Venezuela\n",
      "Atlantis\n",
      "United Kingdom\n",
      "Bangladesh\n",
      "Los Angeles\n",
      "Lucknow, Uttar Pradesh\n",
      "Republic of the Philippines\n",
      "United States\n",
      "Australia\n",
      "San Diego, CA\n",
      "Ph\n",
      "Georgia, USA\n",
      "Merrimack, NH\n",
      "Brisbane, Queensland\n",
      "Lagos, Nigeria\n",
      "World\n",
      "Universe\n",
      "Berlin, Germany\n",
      "San\n",
      "Kerrville, TX\n",
      "Los Angeles, CA\n",
      "Seoul, Republic of Korea\n",
      "Dhaka, Bangladesh\n",
      "Lagos, Nigeria\n",
      "Washington\n",
      "World\n",
      "Nigeria\n",
      "World\n",
      "Blockchain\n",
      "Dhaka\n",
      "World\n",
      "World\n",
      "South East Asia\n",
      "Bacoor Cavite, Philippines\n",
      "East Region, Singapore\n",
      "Republic of the Philippines\n",
      "South East Asia\n",
      "Brazil\n",
      "Dansoman\n",
      "Taipei City, Taiwan\n",
      "Pakistan\n",
      "Virginia Beach, VA\n",
      "Tampa Bay\n",
      "Georgia, USA\n",
      "New York, NY\n",
      "Beijing\n",
      "West Virginia, USA\n",
      "Blockchain\n",
      "Dhaka\n",
      "Green Bay, WI, USA\n",
      "Alabama, USA\n",
      "Irapuato \n",
      "Bogot√°, D.C., Colombia\n",
      "Eden \n",
      "Unknown\n",
      "Portland, OR\n",
      "Philippines\n",
      "South East Asia\n",
      "Canada\n",
      "Bogot√°, D.C., Colombia\n",
      "Stockholm, Sweden\n",
      "Boulder, CO\n",
      "Ife\n",
      "San Diego, CA\n",
      "Brisbane, Queensland\n",
      "Paradise\n",
      "Hong Kong\n",
      "United States\n",
      "Bogot√°, D.C., Colombia\n",
      "The Ringed City\n",
      "Nairobi, Kenya\n",
      "Nairobi, Kenya\n",
      "Bulacan, Central Luzon\n",
      "Chicago, IL\n",
      "South East Asia\n",
      "Australia\n",
      "Bogot√°, D.C., Colombia\n",
      "Philadelphia, PA\n",
      "Middle America\n",
      "Bogot√°, D.C., Colombia\n",
      "Perpignan, France\n",
      "Venice\n",
      "Los Angeles, CA\n",
      "Syria \n",
      "Georgia, USA\n",
      "Stanford\n",
      "+2347066052228 , Nigeria\n",
      "Australia\n",
      "Republic of the Philippines\n",
      "Czech Republic\n",
      "Haridwar, Uttarakhand\n",
      "Makati City, National Capital \n",
      "Bogot√°, D.C., Colombia\n",
      "Czech Republic\n",
      "üåô Moon üåô\n",
      "Bali, Indonesia\n",
      "Australia\n",
      "Lagos State - NIGERIA\n",
      "Bogot√°, D.C., Colombia\n",
      "Japan\n",
      "Liberty City, Fl\n",
      "Bulacan, Central Luzon\n",
      "busan korea\n",
      "New York, USA\n",
      "‚úàÔ∏è\n",
      "California, USA\n",
      "Georgia, USA\n",
      "Antarctica\n",
      "Cebu City\n",
      "Australia \n",
      "United States\n",
      "Republic of the Philippines\n",
      "Bangladesh\n",
      "Barcelona\n",
      "Houston, TX\n",
      "Tokyo, Japan\n",
      "Nashville, Tennessee\n",
      "Tokyo, Japan\n",
      "New York, NY\n",
      "#WolfPack\n",
      "Rancho Santa Margarita\n",
      "Palembang, South Sumatra\n",
      "Bogot√°, D.C., Colombia\n",
      "United Kingdom\n",
      "New York, NY\n",
      "Mackay, Queensland\n",
      "Tokyo, Japan\n",
      "Online\n",
      "Chile\n",
      "California, USA\n",
      "Bogot√°, D.C., Colombia\n",
      "Bangladesh\n",
      "Czech Republic\n",
      "United States\n",
      "peru\n",
      "Australia\n",
      "peru\n",
      "Australia\n",
      "USA\n",
      "NYC-ish\n",
      "Portsmouth, NH \n",
      "British Columbia, Canada\n",
      "World\n",
      "Branch, Arkansas\n",
      "Australia\n",
      "Seoul, Republic of Korea\n",
      "London, England\n",
      "Block Island, RI\n",
      "worldwide\n",
      "Âåó‰∫¨, ‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ\n",
      "Bathurst, New Brunswick, Canada\n",
      "New York City\n",
      "WorldWide\n",
      "Abuja, Nigeria\n",
      "Los Angeles, CA\n",
      "Singapore\n",
      "International\n",
      "Hong Kong\n",
      "Turkey\n",
      "Cape Town, South Africa\n",
      "Shanghai\n",
      "The World\n",
      "Brasil\n",
      "Moon\n",
      " Berlin\n",
      "London, England\n",
      "Florida, USA\n",
      "New York, USA\n",
      "internet\n",
      "Nashville, TN\n",
      "Brasil\n",
      "Los Angeles, CA\n",
      "Crypto Island\n",
      "pittsburgh\n",
      "Brasil\n",
      "Flynn's Arcade\n",
      "new j\n",
      "Los Angeles, CA\n",
      "Cape Town, South Africa\n",
      "Sweden\n",
      "United States\n",
      "www\n",
      "Mariveles, Central Luzon\n",
      "Virginia Beach, VA\n",
      "IIT Delhi, New Delhi\n",
      "Internet\n",
      "Barcelona, Spain\n",
      "Moon\n",
      "Polska\n",
      "Singapore / London\n",
      "USA\n",
      "The World\n",
      "The World\n",
      "Internet\n",
      "USA\n",
      "United States\n",
      "England, United Kingdom\n",
      "Kerrville, TX\n",
      "The Earth\n",
      "Matter Doesn't Matter\n",
      "eua\n",
      "Shambhala\n",
      "Canada\n",
      "The Great Pacific Northwest\n",
      "Cape Town, South Africa\n",
      "Mountain View, CA\n",
      "Australia\n",
      "Mission - San Francisco, CA\n",
      "Ontario, Canada\n",
      "World\n",
      "World\n",
      "World\n",
      "Southern California\n",
      "United States\n",
      "Michigan, USA\n",
      "Bakersfield, CA\n",
      "Boston, MA\n",
      "Lekitaba\n",
      "New Jersey, USA\n",
      "Talty, TX\n",
      "Georgia, USA\n",
      "South East Asia\n",
      "USA\n",
      "Dhaka, Bangladesh\n",
      "The World\n",
      "√ñsterreich\n",
      "India\n",
      "United States\n",
      "Southern California\n",
      "British Columbia, Canada\n",
      "Turkey\n",
      "Montr√©al, Qu√©bec\n",
      "#WolfPack\n",
      "New York, USA\n",
      "Nowhere\n",
      "Austin, Texas\n",
      "Queens, NY\n",
      "United States\n",
      "Jakarta Capital Region\n",
      "National Capital Region, Republic of the Philippines\n",
      "Beverly Hills, CA\n",
      "Australia\n",
      "Minneapolis, MN\n",
      "Online\n",
      "Gloucester, England\n",
      "Sydney\n",
      "Georgia, USA\n",
      "Washington\n",
      "SF\n",
      "Bangkok, Thailand\n",
      "Boston\n",
      "Caracas, Venezuela\n",
      "Austin, TX\n",
      "USA\n",
      "Blockchain\n",
      "Vancouver, Canada üá®üá¶\n",
      "Bath\n",
      "United Kingdom\n",
      "Ulm Deutschland \n",
      "Singapore\n",
      "B·∫Øc Ninh, Vi·ªát Nam\n",
      "Naples, FL\n",
      "Indiana \n",
      "Ireland ? üåé\n",
      "Amazon HQ- Seattle Wa.\n",
      "Blockchain\n",
      "Chicago, IL\n",
      "WORLD\n",
      "ÎåÄÌïúÎØºÍµ≠ ÏÑúÏö∏\n",
      "WORLD\n",
      "Heaven on earth.\n",
      "Earth\n",
      "Friedberg, Deutschland\n",
      "Diyarbakƒ±r, T√ºrkiye\n",
      "Manhattan, NY\n",
      "the cloud\n",
      "San Francisco, CA\n",
      "New York, NY\n",
      "+2347066052228 , Nigeria\n",
      "Atlanta, GA\n",
      "Lagos, Nigeria\n",
      "Republic of the Philippines\n",
      "Osun, Nigeria\n",
      "United States\n",
      "Los Angeles, CA\n",
      "Bangkok, Thailand\n",
      "Singapore\n",
      "Nairobi,Kenya\n",
      "United Kingdom\n",
      "World\n",
      "Benin-City, Nigeria\n",
      "World\n",
      "World\n",
      "World\n",
      "Hollywood, FL\n",
      "Michigan\n",
      "California, USA\n",
      "Barcelona, Espa√±a\n",
      "Port Harcourt, Nigeria\n",
      "Perth\n",
      "Brooklyn, NY\n",
      "Seattle, WA\n",
      "Australia, UK\n",
      "San Angelo, TX\n",
      "Berlin, Germany\n",
      "brasil\n",
      "world\n",
      "Albany NY\n",
      "Bengaluru, India\n",
      "Distributed\n",
      "South Africa\n",
      "Bengaluru, India\n",
      "Seattle\n",
      "Brisbane, Queensland\n",
      "Anaheim, CA\n",
      "World\n",
      "Atlantis\n",
      "Texas, USA\n",
      "United States\n",
      "USA\n",
      "Haiti\n",
      "Cambodia\n",
      "United Kingdom\n",
      "United States\n",
      "Thailand\n",
      "Mars 20-1A\n",
      "New York, NY\n",
      "California, USA\n",
      "Toronto, Canada | üá®üá¶üáµüá∞üá´üáØ\n",
      "London, England\n",
      "Czech Republic\n",
      "Chicago, Illinois \n",
      "longview, tx\n",
      "United States\n",
      "Vietnam\n",
      "New York, NY\n",
      "London, UK\n",
      "Lake Stevens, WA\n",
      "The World\n",
      "Bentonville, AR\n",
      "6 | Van City | Gidi\n",
      "Bathurst, New Brunswick, Canada\n",
      "Lagos Nigeria \n",
      "Georgia, USA\n",
      "London,UK\n",
      "Melbourne, Victoria\n",
      "Panama\n",
      "England, United Kingdom\n",
      "Central Mississippi\n",
      "United States\n",
      "Nakano-ku, Tokyo\n",
      "Melbourne, Australia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nigeria\n",
      "Los Angeles, CA\n",
      "Berlin, Germany\n",
      "Lagos, Nigeria\n",
      "Kingman, AZ\n",
      "DC\n",
      "Texas, USA\n",
      "Worldwide\n",
      "London, England\n",
      "California, USA\n",
      "Hong Kong\n",
      "Maharashtra, India\n",
      "San Francisco, CA\n",
      "Maharashtra, India\n",
      "Denver, CO\n",
      "Japan\n",
      "New York, USA\n",
      "Myrtle Beach, SC USA \n",
      "Chicago\n",
      "South Carolina, USA\n",
      "Home\n",
      "Hunts Point Market, Bronx\n",
      "Miami, FL\n",
      "Guarenas\n",
      "United States\n",
      "New York, New York\n",
      "Australia\n",
      "EMEA\n",
      "Minneapolis, MN\n",
      "Planet Earth \n",
      "Miami\n",
      "euro\n",
      "Cairo \n",
      "London, England\n",
      " Alabama\n",
      "‰∏äÊµ∑, ‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ\n",
      "Lagos, Nigeria\n",
      "San Diego\n",
      "San Diego\n",
      "Water\n",
      "Mississauga, Ontario\n",
      "Philippines\n",
      "California, USA\n",
      "Bakersfield, CA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Go through all tweets and add locations to 'coordinates' dictionary\n",
    "coordinates = {'location': [], 'latitude': [], 'longitude': []}\n",
    "\n",
    "\n",
    "for count, user_loc in enumerate(tweets2.user_loc):\n",
    "    try:\n",
    "        location = geolocator.geocode(user_loc)\n",
    "        \n",
    "        # If coordinates are found for location\n",
    "        \n",
    "        if isinstance(location.latitude, numbers.Number):\n",
    "            #print(user_loc)\n",
    "            coordinates['location'].append(user_loc)\n",
    "            coordinates['latitude'].append(location.latitude)\n",
    "            coordinates['longitude'].append(location.longitude)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coordinates['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(coordinates['location'])):\n",
    "     with open(file_location, 'a', encoding='utf-8') as file:\n",
    "         writer = csv.writer(file)\n",
    "         writer.writerow([coordinates['location'][i], coordinates['latitude'][i], coordinates['longitude'][i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_location = pd.read_csv(file_location)\n",
    "user_location1 = user_location.dropna()\n",
    "len(list(user_location1[\"latitude\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save cleaned location in CSV file\n",
    "user_location1.to_csv(\"twitter_loc.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and center a GoogleMapPlotter object to show our map\n",
    "gmap = gmplot.GoogleMapPlotter(30, 0, 3)\n",
    "\n",
    "# Insert points on the map passing a list of latitudes and longitudes\n",
    "#gmap.heatmap(coordinates['latitude'], coordinates['longitude'], radius=20)\n",
    "gmap.heatmap(list(user_location1['latitude']), list(user_location1['longitude']), radius=20)\n",
    "\n",
    "# Save the map to html file\n",
    "gmap.draw(\"python_heatmap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
