{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/shujinkou/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/shujinkou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/shujinkou/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/shujinkou/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# api wrapper\n",
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "#natural language processing libraries\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "\n",
    "#textblob imports\n",
    "from textblob import Blobber\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import FastNPExtractor\n",
    "tb = Blobber(analyzer=NaiveBayesAnalyzer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift API wrapper query\n",
    "    * automatic rate limiting\n",
    "    * faster queries as it doesn't actually directly query reddit's servers\n",
    "    * better searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift submission API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all threads from the bitcoinmarket subreddit, gen is a generator \n",
    "\n",
    "start_epoch=int(dt.datetime(2013, 1, 1).timestamp())\n",
    "gen = api.search_submissions(after=start_epoch,\n",
    "        subreddit='bitcoinmarkets', #specify the subforum\n",
    "        sort = 'asc',\n",
    "            # filter=['url','author', 'title', 'subreddit'], # choose which dictionary keys to display\n",
    "        limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the generator to create a submission array\n",
    "# x.d_ is a method that converts each submission object to a dictionary\n",
    "results = list(map(lambda x: x.d_, gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv('bitcionmarkets_threads.csv')\n",
    "df.to_json('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift comment API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for comments in the bitcoinmarkets subredidt\n",
    "\n",
    "def query_comments(search, max_response_cache):\n",
    "    \"\"\" Finds comments within a particular subreddit containing a phrase\n",
    "    \n",
    "    Keyword arguments:\n",
    "    search (string) -- phrase to search for\n",
    "    max_response_cache(int) -- maximum number of comments to retreive\n",
    "\n",
    "    \"\"\"\n",
    "    gen = api.search_comments(q=search, subreddit='bitcoinmarkets')\n",
    "    cache = []\n",
    "\n",
    "    for c in gen:\n",
    "        cache.append(c.d_)\n",
    "\n",
    "        # Omit this test to actually return all results\n",
    "        if len(cache) >= max_response_cache:\n",
    "            break\n",
    "            \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25400\n",
      "15797\n"
     ]
    }
   ],
   "source": [
    "bullish = query_comments(\"bullish\", 100000)\n",
    "bearish = query_comments(\"bearish\", 100000)\n",
    "print(len(bullish))\n",
    "print(len(bearish))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing. Sentiment and noun phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(comment):\n",
    "    \"\"\" Returns texblob sentiment analysis for a comment's body\n",
    "    \n",
    "    Keyword arguments:\n",
    "    comment (dict) -- element from a query_comments array\n",
    "    \"\"\"\n",
    "    return TextBlob(comment['body']).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "pool = ThreadPool()\n",
    "# utalize all cores\n",
    "# applies sentiment analysis to each comment\n",
    "bullish_sentiments = pool.map(lambda x: tb(x['body']).sentiment, bullish)\n",
    "#close the pool and wait for the work to finish\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_polarity = list(map(lambda x: x.p_pos, bullish_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm insanely bullish about the price in the medium term and I'd love to leverage those gains up. But im a bit thick and even if I wasnt I dont think low leverage is a good idea at the moment. 60/40 seems crazy irresponsible. But I can get a lot of Yolos for that price even with randomly selected my way in I feel like I'm much more likely to come right. One random shot every week for the next few months at say 50 times seems very likely to come right at least once, and maybe right a lot of times. Of course the price actually needs to rise, but if it drops for months low leverage is dead anyway. I'm making excuses for greed, but I wonder if there is not a way to make this about as safe as holding alone. Say 10 percent fiat ashedge/rebut at the bottom. And a random leveraged punt every week for a year. So much\n",
      "\n",
      "Upside, and the odds seem good? I'm a bit clueless though. Can still get stopped out I guess. What are the odds of Getting unlucky is the price is rising at a good rate? If I was doing this it would be with tiny money and only building if there was profit.  Foolproof is not meaningful- I'm looking for good odds for a crappy trader without spending all day chart watching.\n",
      "\n",
      "\n",
      "I have some other questions for the wise and benevolent,  but I feel like this might be useful as a standalone to others if correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# rargmin eturns comment with the lowest polarity\n",
    "print(bullish[np.argmin(bullish_polarity)]['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04/21/2013, 12:01:00'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.fromtimestamp(bearish[-1]['created_utc']).strftime(\"%m/%d/%Y, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example noun phrase extraction\n",
    "* This can be used for a bubble word frequency chart, or a cooccurance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomo = query_comments(\"FOMO\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate all the text in each comment into a single string to apply textblob\n",
    "all_fomo = ' '.join(list(map(lambda x: x['body'], fomo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are two models for extracting noun phrases, Conll is faster, but less accurate\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "#extractor = FastNPExtractor()\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "# tb_noun is a \"blobber\" convience function that converts strings into textblob objects\n",
    "tb_noun = Blobber(np_extractor=extractor)\n",
    "df = pd.DataFrame(FreqDist(tb_noun(all_fomo).noun_phrases).most_common(50))\n",
    "df = df.rename(columns = {0: \"word\", 1: \"frequency\"})\n",
    "df.to_json('./bubble_chart/data/word_frequency.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
