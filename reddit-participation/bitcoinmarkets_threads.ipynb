{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "#natural language processing libraries\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "import time\n",
    "from textblob import Blobber\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "tb = Blobber(analyzer=NaiveBayesAnalyzer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift API wrapper query\n",
    "    * automatic rate limiting\n",
    "    * faster queries as it doesn't actually directly query reddit's servers\n",
    "    * better searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift submission API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all threads from the bitcoinmarket subreddit, gen is a generator \n",
    "\n",
    "start_epoch=int(dt.datetime(2013, 1, 1).timestamp())\n",
    "gen = api.search_submissions(after=start_epoch,\n",
    "        subreddit='bitcoinmarkets', #specify the subforum\n",
    "        sort = 'asc',\n",
    "            # filter=['url','author', 'title', 'subreddit'], # choose which dictionary keys to display\n",
    "        limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the generator to create a submission array\n",
    "# x.d_ is a method that converts each submission object to a dictionary\n",
    "results = list(map(lambda x: x.d_, gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv('bitcionmarkets_threads.csv')\n",
    "df.to_json('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift comment API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for comments in the bitcoinmarkets subredidt\n",
    "\n",
    "def query_comments(search):\n",
    "    \"\"\" Finds comments within a particular subreddit containing a phrase\n",
    "    \n",
    "    Keyword arguments:\n",
    "    search (string) -- phrase to search for\n",
    "\n",
    "    \"\"\"\n",
    "    gen = api.search_comments(q=search, subreddit='bitcoinmarkets')\n",
    "    max_response_cache = 100000\n",
    "    cache = []\n",
    "\n",
    "    for c in gen:\n",
    "        cache.append(c.d_)\n",
    "\n",
    "        # Omit this test to actually return all results\n",
    "        if len(cache) >= max_response_cache:\n",
    "            break\n",
    "            \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25389\n",
      "15792\n"
     ]
    }
   ],
   "source": [
    "bullish = query_comments(\"bullish\")\n",
    "bearish = query_comments(\"bearish\")\n",
    "print(len(bullish))\n",
    "print(len(bearish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(comment):\n",
    "    \"\"\" Returns texblob sentiment analysis for a comment's body\n",
    "    \n",
    "    Keyword arguments:\n",
    "    comment (dict) -- element from a query_comments array\n",
    "    \"\"\"\n",
    "    return TextBlob(comment['body']).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "pool = ThreadPool()\n",
    "# utalize all cores\n",
    "bullish_sentiments = pool.map(lambda x: tb(x['body']).sentiment, bullish)\n",
    "#close the pool and wait for the work to finish\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_polarity = list(map(lambda x: x.p_pos, bullish_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm insanely bullish about the price in the medium term and I'd love to leverage those gains up. But im a bit thick and even if I wasnt I dont think low leverage is a good idea at the moment. 60/40 seems crazy irresponsible. But I can get a lot of Yolos for that price even with randomly selected my way in I feel like I'm much more likely to come right. One random shot every week for the next few months at say 50 times seems very likely to come right at least once, and maybe right a lot of times. Of course the price actually needs to rise, but if it drops for months low leverage is dead anyway. I'm making excuses for greed, but I wonder if there is not a way to make this about as safe as holding alone. Say 10 percent fiat ashedge/rebut at the bottom. And a random leveraged punt every week for a year. So much\n",
      "\n",
      "Upside, and the odds seem good? I'm a bit clueless though. Can still get stopped out I guess. What are the odds of Getting unlucky is the price is rising at a good rate? If I was doing this it would be with tiny money and only building if there was profit.  Foolproof is not meaningful- I'm looking for good odds for a crappy trader without spending all day chart watching.\n",
      "\n",
      "\n",
      "I have some other questions for the wise and benevolent,  but I feel like this might be useful as a standalone to others if correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(bullish[np.argmin(bullish_polarity)]['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.7249996645149416, p_neg=0.27500033548505376)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bullish_sentiments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04/21/2013, 12:01:00'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.fromtimestamp(bearish[-1]['created_utc']).strftime(\"%m/%d/%Y, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
